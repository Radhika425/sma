#https://www.kaggle.com/code/paramarthasengupta/eda-zomato-hyderabad/input?select=Restaurant+reviews.csv

!pip install pandas nltk wordcloud matplotlib scikit-learn
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from collections import Counter
import re
# Download necessary NLTK data
nltk.download('stopwords')

# Load the dataset
df = pd.read_csv('yelp.csv', nrows=50, on_bad_lines='skip', encoding='utf-8')  # Make sure to provide correct path

# Check the first few rows of the dataset to understand its structure
print(df.head())

# Assuming the column with reviews is 'Review', but replace with actual column name if needed
reviews = df['Review'].dropna()  # Drop any NaN values
# Text cleaning function
def clean_text(text):
    # Remove non-alphabetical characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = text.lower()  # Convert to lowercase
    text = text.strip()  # Remove leading/trailing whitespace
    return text

# Apply text cleaning
reviews_cleaned = reviews.apply(clean_text)
# Step 1: Generate a Word Cloud
all_reviews = ' '.join(reviews_cleaned)  # Combine all reviews into a single string

# Define stopwords
stop_words = set(stopwords.words('english'))

# Generate the word cloud
wordcloud = WordCloud(stopwords=stop_words, background_color='white', width=800, height=400).generate(all_reviews)

# Plot the Word Cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Word Cloud - Restaurant Reviews")
plt.show()

# Step 2: N-gram Analysis (Bigrams in this case)
vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')
X = vectorizer.fit_transform(reviews_cleaned)

# Convert n-gram counts to DataFrame
ngram_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())

# Sum counts for each bigram
bigram_counts = ngram_df.sum(axis=0).sort_values(ascending=False)

# Print top 10 most frequent bigrams (could represent common complaints)
print("\nTop 10 Bigrams (Common Complaints):")
print(bigram_counts.head(10))

# Step 3: Identify common complaint areas
complaints = bigram_counts.head(10).index
print("\nCommon Complaint Areas (Bigrams):")
for complaint in complaints:
    print(f"- {complaint}")


# Example: Filter only low-rated reviews if 'Rating' column exists
negative_reviews = df[df['Rating'] <= 2]['Review'].dropna().apply(clean_text)

from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

sia = SentimentIntensityAnalyzer()

# Compute sentiment scores
df['sentiment'] = df['Review'].astype(str).apply(lambda x: sia.polarity_scores(x)['compound'])

# Filter for negative sentiment
negative_reviews = df[df['sentiment'] <= -0.2]['Review'].dropna().apply(clean_text)
X = vectorizer.fit_transform(negative_reviews)
# Convert n-gram counts to DataFrame
ngram_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())

# Sum counts for each bigram
bigram_counts = ngram_df.sum(axis=0).sort_values(ascending=False)

# Print top 10 most frequent bigrams (could represent common complaints)
print("\nTop 10 Bigrams (Common Complaints):")
print(bigram_counts.head(10))

# Step 3: Identify common complaint areas
complaints = bigram_counts.head(10).index
print("\nCommon Complaint Areas (Bigrams):")
for complaint in complaints:
    print(f"- {complaint}")
