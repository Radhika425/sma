# https://snap.stanford.edu/data/higgs-twitter.html
import networkx as nx
import gzip
import matplotlib.pyplot as plt
import random

# Initialize a directed graph
G = nx.DiGraph()

# Step 1: Load the retweet network (Higgs Twitter dataset)
with gzip.open('higgs-retweet_network.edgelist.gz', 'rt') as f:
    for line in f:
        parts = line.strip().split()
        if len(parts) == 3:
            userA, userB, weight = parts
            G.add_edge(userA, userB, weight=int(weight))

print(f"Total nodes in full graph: {G.number_of_nodes()}, Total edges: {G.number_of_edges()}")

# Step 2: Sample the graph to improve performance (e.g., 10% of nodes)
sample_fraction = 0.02
sampled_nodes = random.sample(list(G.nodes), int(len(G.nodes) * sample_fraction))
G_sampled = G.subgraph(sampled_nodes).copy()

print(f"Sampled nodes: {G_sampled.number_of_nodes()}, Sampled edges: {G_sampled.number_of_edges()}")

# Step 3: Compute centrality measures on the sampled subgraph

# In-degree centrality
in_degree_centrality = nx.in_degree_centrality(G_sampled)

# Betweenness centrality (much faster on smaller graph)
betweenness_centrality = nx.betweenness_centrality(G_sampled, k=100, seed=42)  # Approximation using 100 random nodes

# PageRank
pagerank = nx.pagerank(G_sampled, alpha=0.85)


# Closeness centrality
closeness_centrality = nx.closeness_centrality(G_sampled)

# Eigenvector centrality (note: works on strongly connected components)
try:
    eigenvector_centrality = nx.eigenvector_centrality(G_sampled, max_iter=1000)
except nx.PowerIterationFailedConvergence:
    print("Eigenvector centrality did not converge. Trying on largest strongly connected component...")
    largest_scc = max(nx.strongly_connected_components(G_sampled), key=len)
    G_scc = G_sampled.subgraph(largest_scc).copy()
    eigenvector_centrality = nx.eigenvector_centrality(G_scc, max_iter=1000)
    print("Successfully computed eigenvector centrality on largest SCC.")


# Sort nodes by centralities
top_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]
sorted_in_degree = sorted(in_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]
sorted_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]
sorted_closeness = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]
sorted_eigenvector = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)[:10]

# Step 4: Display the top 10 influencers
print("\nTop 10 Influencers by In-degree Centrality:")
for user, score in sorted_in_degree:
    print(f"User: {user}, In-degree Centrality: {score:.6f}")

print("\nTop 10 Influencers by Betweenness Centrality:")
for user, score in sorted_betweenness:
     print(f"User: {user}, Betweenness Centrality: {score:.6f}")

print("\nTop 10 Influencers by PageRank:")
for user, score in top_pagerank:
    print(f"User: {user}, PageRank Score: {score:.6f}")

print("\nTop 10 Influencers by Closeness Centrality:")
for user, score in sorted_closeness:
    print(f"User: {user}, Closeness Centrality: {score:.6f}")

print("\nTop 10 Influencers by Eigenvector Centrality:")
for user, score in sorted_eigenvector:
    print(f"User: {user}, Eigenvector Centrality: {score:.6f}")

# Step 5: Visualize the sampled network
plt.figure(figsize=(10, 10))
pos = nx.kamada_kawai_layout(G_sampled)

nx.draw_networkx_nodes(G_sampled, pos, node_size=10, node_color='blue')
if G_sampled.number_of_edges() < 500:
    nx.draw_networkx_edges(G_sampled, pos, alpha=0.1, edge_color='gray')

plt.title("Sampled Retweet Network (Optimized)")
plt.axis('off')
plt.show()
